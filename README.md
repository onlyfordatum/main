# üìò Orquestrador de Pipeline de Dados - Databricks

Este notebook (nb_pipeline_orchestrator) tem como objetivo coordenar de forma sequencial e automatizada o processo de ingest√£o, tratamento e transforma√ß√£o de dados, seguindo a arquitetura em camadas Medallion: Bronze ‚Üí Silver ‚Üí Gold. Foi criado um scheduler job no Databricks que executa este fluxo diariamente √† meia-noite, garantindo a atualiza√ß√£o cont√≠nua e programada dos dados.

---

## üìå 1. Defini√ß√£o dos Objetos de Ingest√£o

O pipeline come√ßa com a defini√ß√£o da lista de datasets que ser√£o ingeridos. Essa lista permite a expans√£o do processo para m√∫ltiplos arquivos com a mesma estrutura ou origem.

```python
ingestion_objects = ['global-super-store-dataset']
```

---

## üì• 2. Ingest√£o via API do Kaggle

Para cada objeto da lista, √© executado o notebook `nb_ingestion_call_api`, respons√°vel por baixar os dados diretamente da API do Kaggle.

Este processo √© encapsulado em um bloco `try/except`, garantindo toler√¢ncia a falhas.

```python
./ingestion/nb_ingestion_call_api
```

---

## ü™ô 3. Camada Bronze

Os dados brutos obtidos s√£o enviados para a camada **Bronze**, onde s√£o armazenados sem altera√ß√µes, funcionando como uma r√©plica segura dos dados de origem.

```python
./medalion/bronze/nb_bronze
```

---

## ü•à 4. Camada Silver

Na camada **Silver**, os dados passam por tratamento, limpeza e padroniza√ß√£o. Isso garante integridade e estrutura adequada para uso anal√≠tico.

```python
./medalion/silver/nb_silver_global_superstore
```

---

## ü•á 5. Camada Gold

A camada **Gold** √© respons√°vel pela transforma√ß√£o dos dados em insights. S√£o aplicadas agrega√ß√µes, c√°lculos e m√©tricas espec√≠ficas para gerar outputs prontos para consumo.

Notebooks executados nesta etapa:

```python
[
  './medalion/gold/nb_gold_annual_metrics_delta',
  './medalion/gold/nb_gold_annual_metrics_parquet',
  './medalion/gold/nb_gold_category_delta',
  './medalion/gold/nb_gold_category_parquet',
  './medalion/gold/nb_gold_total_order_value_delta',
  './medalion/gold/nb_gold_total_order_value_parquet'
]
```

---

## ‚öôÔ∏è Mecanismo de Execu√ß√£o

Todos os notebooks s√£o chamados com `dbutils.notebook.run`, com timeout definido e par√¢metros de entrada quando necess√°rio. Em caso de falhas, o erro √© capturado e a execu√ß√£o √© encerrada com uma mensagem clara.

---
# Projeto de Ingest√£o de Dados do Global Superstore

## Vis√£o Geral

Este projeto implementa um pipeline de ingest√£o de dados no Databricks para extrair, processar e carregar o conjunto de dados **Global Superstore Dataset** do Kaggle em uma tabela estruturada na camada Bronze de uma arquitetura de data lakehouse. O pipeline √© projetado para lidar com dados no formato CSV, utilizando o PySpark para defini√ß√£o de esquemas e a API do Kaggle para obten√ß√£o do dataset.

## Estrutura do Projeto

- **Configura√ß√£o de Esquemas**: Define o esquema para o dataset Global Superstore e os par√¢metros de ingest√£o.
- **L√≥gica de Ingest√£o**: Implementa o processo de extra√ß√£o de dados do Kaggle, c√≥pia de configura√ß√µes seguras e carga dos dados em uma tabela no cat√°logo Bronze.

## Configura√ß√£o do Ambiente

### Pr√©-requisitos
- Databricks Runtime com suporte ao PySpark.
- Biblioteca `kaggle` instalada (`%pip install kaggle`).
- Credenciais de API do Kaggle configuradas em um arquivo JSON seguro.
- Permiss√µes adequadas para criar cat√°logos, esquemas e volumes no Databricks.

### Estrutura de Diret√≥rios
- `/Volumes/bronze/ingestion/config/`: Armazena o arquivo de configura√ß√£o da API do Kaggle (`kaggle.json`).
- `/Volumes/bronze/ingestion/landing_zone/`: Diret√≥rio onde os arquivos CSV baixados s√£o armazenados.

## Configura√ß√£o de Esquemas

O esquema do dataset **Global Superstore** √© definido em um notebook auxiliar (`nb_schemas`) usando `StructType` do PySpark. 

### Par√¢metros de Ingest√£o
Os par√¢metros de ingest√£o s√£o configurados no dicion√°rio `_schemas` e incluem:
- **Fonte de Dados**:
  - API: Kaggle
  - Dataset: `apoorvaappz/global-super-store-dataset`
  - Nome do arquivo: `Global_Superstore2.csv`
  - Formato: CSV
  - Local de origem: `/Volumes/bronze/ingestion/landing_zone/`
- **Destino**:
  - Cat√°logo: `bronze`
  - Esquema: `ingestion`
  - Tabela: `global_superstore`

## L√≥gica de Ingest√£o

O processo de ingest√£o segue as seguintes etapas:

1. **Cria√ß√£o de Estruturas no Databricks**:
   - Cria√ß√£o do cat√°logo `bronze`, esquema `ingestion` e volumes `config` e `landing_zone`, se n√£o existirem.
   ```sql
   CREATE CATALOG IF NOT EXISTS bronze;
   CREATE SCHEMA IF NOT EXISTS bronze.ingestion;
   CREATE VOLUME IF NOT EXISTS bronze.ingestion.config;
   CREATE VOLUME IF NOT EXISTS bronze.ingestion.landing_zone;
   ```

2. **Configura√ß√£o da API do Kaggle**:
   - Um arquivo JSON com credenciais da API (`kaggle.json`) √© criado e armazenado em `/Volumes/bronze/ingestion/config/`.
   - O arquivo √© copiado para o diret√≥rio `~/.kaggle/` com permiss√µes restritas (`0o600`) para seguran√ßa.
   ```python
   shutil.copy(source_path, destination_path)
   os.chmod(destination_path, 0o600)
   ```

3. **Download do Dataset**:
   - A biblioteca `kaggle` √© utilizada para baixar o dataset e descompact√°-lo diretamente no diret√≥rio `/Volumes/bronze/ingestion/landing_zone/`.
   ```python
   api.dataset_download_files(src_api_dataset, path="/Volumes/bronze/ingestion/landing_zone/", unzip=True)
   ```

4. **Valida√ß√£o**:
   - Os arquivos baixados s√£o listados para verificar a integridade do processo.
   ```python
   display(dbutils.fs.ls('/Volumes/bronze/ingestion/landing_zone/'))
   ```

# Camada Bronze - Ingest√£o do Global Superstore Dataset

## Vis√£o Geral

Este projeto implementa a camada Bronze de um pipeline de dados no Databricks, respons√°vel por carregar os dados brutos do **Global Superstore Dataset** (obtidos via API do Kaggle) em uma tabela Delta no cat√°logo `bronze`. A camada Bronze serve como a camada inicial de armazenamento, preservando os dados em seu formato bruto com m√≠nimas transforma√ß√µes, garantindo rastreabilidade e consist√™ncia para pipelines downstream.

## Objetivo

- Carregar os dados brutos do arquivo CSV (`Global_Superstore2.csv`) para uma tabela Delta.
- Adicionar um carimbo de ingest√£o (`ingestion_timestamp`) para rastreamento.
- Padronizar nomes de colunas, substituindo caracteres especiais por underscores e convertendo para letras min√∫sculas.
- Persistir os dados na tabela `bronze.ingestion.global_superstore` no formato Delta.
- Remover os arquivos de origem da zona de landing ap√≥s a persist√™ncia.

## Estrutura do Projeto

- **Configura√ß√£o de Esquemas**: Reutiliza as defini√ß√µes de esquema e par√¢metros do notebook `nb_schemas` (definido na camada de ingest√£o).
- **Processamento na Camada Bronze**: L√™ o arquivo CSV, aplica transforma√ß√µes b√°sicas e persiste os dados em uma tabela Delta.

## Pr√©-requisitos

- Databricks Runtime com suporte ao PySpark e Delta Lake.
- Notebook `nb_schemas` configurado com as defini√ß√µes de esquema e par√¢metros de ingest√£o.
- Arquivo CSV (`Global_Superstore2.csv`) dispon√≠vel em `/Volumes/bronze/ingestion/landing_zone/`.
- Permiss√µes adequadas para criar cat√°logos, esquemas e tabelas no Databricks.

## Configura√ß√£o

### Par√¢metros de Ingest√£o
Os par√¢metros s√£o herdados do notebook `nb_schemas` e incluem:

- **Fonte de Dados**:
  - API: `kaggle`
  - Dataset: `apoorvaappz/global-super-store-dataset`
  - Nome do arquivo: `Global_Superstore2.csv`
  - Formato: `csv`
  - Local de origem: `/Volumes/bronze/ingestion/landing_zone/`
- **Destino**:
  - Cat√°logo: `bronze`
  - Esquema: `ingestion`
  - Tabela: `global_superstore`

O esquema do dataset √© definido como um `StructType` com 24 campos, todos do tipo `StringType`, conforme detalhado no notebook de ingest√£o.

## L√≥gica do Pipeline

O processo da camada Bronze segue as seguintes etapas:

1. **Carregamento de Configura√ß√µes**:
   - O notebook `nb_schemas` √© executado para obter o esquema e os par√¢metros de ingest√£o.
   ```python
   %run ./../../ingestion/structs/nb_schemas
   ```

2. **Gera√ß√£o do Carimbo de Ingest√£o**:
   - Um carimbo de tempo (`ingestion_timestamp`) √© gerado no fuso hor√°rio de S√£o Paulo (`America/Sao_Paulo`) no formato `YYYYMMDDHHMMSS`.
   ```python
   ingestion_timestamp = datetime.now(pytz.timezone('America/Sao_Paulo')).strftime('%Y%m%d%H%M%S')
   ```

3. **Identifica√ß√£o do Arquivo de Origem**:
   - O pipeline verifica os arquivos no diret√≥rio `/Volumes/bronze/ingestion/landing_zone/` e seleciona aquele que cont√©m o nome `Global_Superstore2.csv`.
   ```python
   for file in dbutils.fs.ls(src_file_internal_location):
       if file.name.__contains__(src_file_name_contains):
           source_file = file.path.replace("dbfs:","")
   ```

4. **Leitura do Arquivo CSV**:
   - O arquivo CSV √© lido utilizando o formato `csv`, com op√ß√µes de separador (`,`), cabe√ßalho (`true`) e o esquema predefinido.
   ```python
   df = spark.read.format(src_file_format).option('sep', ',').option('header', 'true').schema(schema).load(source_file)
   ```

5. **Transforma√ß√µes B√°sicas**:
   - Os nomes das colunas s√£o padronizados, substituindo caracteres especiais (espa√ßos, v√≠rgulas, etc.) por underscores e convertendo para letras min√∫sculas.
   - Uma coluna `ingestion_timestamp` √© adicionada com o carimbo de ingest√£o (tipo `long`).
   ```python
   for col_name in df.columns:
       new_name = re.sub(r"[ ,;{}\(\)\n\t=]", "_", col_name.lower())
       if col_name != new_name:
           df = df.withColumnRenamed(col_name, new_name)
   df = df.withColumn("ingestion_timestamp", f.lit(ingestion_timestamp).cast("long"))
   ```

6. **Cria√ß√£o de Estruturas no Databricks**:
   - O cat√°logo `bronze` e o esquema `ingestion` s√£o criados, se ainda n√£o existirem.
   ```sql
   CREATE CATALOG IF NOT EXISTS bronze;
   CREATE SCHEMA IF NOT EXISTS bronze.ingestion;
   ```

7. **Persist√™ncia dos Dados**:
   - Os dados s√£o salvos como uma tabela Delta no modo `append`, garantindo que novos dados sejam adicionados sem sobrescrever registros existentes.
   ```python
   df.write.format("delta").mode("append").saveAsTable(f"{dest_catalog_name}.{dest_schema_name}.{dest_table_name}")
   ```

8. **Limpeza da Zona de Landing**:
   - Ap√≥s a persist√™ncia, o arquivo de origem √© removido da pasta `/Volumes/bronze/ingestion/landing_zone/` para evitar redund√¢ncia.
   ```python
   for file in dbutils.fs.ls('/Volumes/bronze/ingestion/landing_zone/'):
       if file.name.__contains__(src_file_name_contains.replace('.csv', '')):
           dbutils.fs.rm(file.path.replace('dbfs:', ''))
   ```

## Estrutura da Tabela Bronze

A tabela `bronze.ingestion.global_superstore` cont√©m todos os 24 campos do dataset original, com nomes de colunas padronizados (ex.: `row_id`, `order_date`, `product_name`) e a coluna adicional `ingestion_timestamp` para rastreamento.

## Notas

- **Integridade dos Dados**: A camada Bronze mant√©m os dados brutos com m√≠nimas transforma√ß√µes, garantindo que pipelines downstream possam realizar processamentos adicionais.
- **Escalabilidade**: O pipeline pode ser adaptado para outros datasets, ajustando o esquema e os par√¢metros no notebook `nb_schemas`.
- **Manuten√ß√£o**: Monitore a pasta de landing para garantir que apenas os arquivos esperados sejam processados e removidos.

---

# Camada Silver - Refinamento do Global Superstore Dataset

## Vis√£o Geral

Este projeto implementa a camada Silver de um pipeline de dados no Databricks, respons√°vel por refinar os dados brutos da tabela `bronze.ingestion.global_superstore` e armazen√°-los em uma tabela Delta na camada Silver (`silver.refined.global_superstore`). A camada Silver aplica transforma√ß√µes para padronizar tipos de dados, tratar valores nulos e limpar inconsist√™ncias, garantindo maior qualidade e usabilidade para an√°lises e pipelines downstream.

## Objetivo

- Filtrar os dados mais recentes da camada Bronze com base no `ingestion_timestamp`.
- Converter colunas para tipos de dados apropriados (ex.: `date`, `int`, `decimal`).
- Tratar valores nulos e inconsist√™ncias, como strings n√£o num√©ricas na coluna `sales`.
- Persistir os dados refinados na tabela Delta `silver.refined.global_superstore` no modo `overwrite`.

## Estrutura do Projeto

- **Configura√ß√£o de Esquemas**: Reutiliza os par√¢metros do notebook `nb_schemas` para definir a origem (camada Bronze) e o destino (camada Silver).
- **Processamento na Camada Silver**: Aplica transforma√ß√µes de dados, incluindo convers√£o de tipos, tratamento de nulos e limpeza de strings.


## Configura√ß√£o

### Par√¢metros de Processamento
Os par√¢metros s√£o herdados do notebook `nb_schemas` e adaptados para a camada Silver:

- **Origem (Camada Bronze)**:
  - Cat√°logo: `bronze`
  - Esquema: `ingestion`
  - Tabela: `global_superstore`
- **Destino (Camada Silver)**:
  - Cat√°logo: `silver`
  - Esquema: `refined`
  - Tabela: `global_superstore`

### Estrutura da Tabela de Origem
A tabela `bronze.ingestion.global_superstore` cont√©m 24 campos de dados brutos (todos como `StringType`) e uma coluna `ingestion_timestamp` (tipo `long`).

## L√≥gica do Pipeline

O processo da camada Silver segue as seguintes etapas:

1. **Carregamento de Configura√ß√µes**:
   - O notebook `nb_schemas` √© executado para obter os par√¢metros de origem e destino.
   ```python
   %run ./../../ingestion/structs/nb_schemas
   ```

2. **Cria√ß√£o de Estruturas no Databricks**:
   - O cat√°logo `silver` e o esquema `refined` s√£o criados, se ainda n√£o existirem.
   ```sql
   CREATE CATALOG IF NOT EXISTS silver;
   CREATE SCHEMA IF NOT EXISTS silver.refined;
   ```

3. **Filtragem dos Dados Mais Recentes**:
   - O pipeline identifica o `ingestion_timestamp` mais recente da tabela Bronze e filtra os dados correspondentes.
   ```python
   filter_max = spark.table(f"{src_catalog_name}.{src_schema_name}.{src_table_name}").select(f.max("ingestion_timestamp")).collect()[0][0]
   df = spark.table(f"{src_catalog_name}.{src_schema_name}.{src_table_name}").filter(f"""ingestion_timestamp = {filter_max}""")
   ```

4. **Transforma√ß√µes de Dados**:
   - **Colunas de Data (`order_date`, `ship_date`)**:
     - Convertidas para o tipo `date` usando o formato `dd-MM-yyyy`.
     - Valores nulos s√£o substitu√≠dos por `1900-01-01`.
     ```python
     for col in ['order_date', 'ship_date']:
         df = df.withColumn(col, f.trim(f.to_date(f.col(col), 'dd-MM-yyyy')))
         df = df.withColumn(col, f.when(f.col(col).isNull(), f.lit('1900-01-01').cast('date')).otherwise(f.col(col)))
     ```
   - **Colunas Inteiras (`row_id`)**:
     - Convertidas para o tipo `int`.
     - Valores nulos s√£o substitu√≠dos por `0`.
     ```python
     for col in ['row_id']:
         df = df.withColumn(col, f.col(col).cast('int'))
         df = df.withColumn(col, f.when(f.col(col).isNull(), f.lit(0).cast('int')).otherwise(f.col(col)))
     ```
   - **Colunas Decimais (`quantity`, `discount`, `profit`, `shipping_cost`)**:
     - Convertidas para o tipo `decimal(12,2)`.
     - Valores nulos s√£o substitu√≠dos por `0.00`.
     ```python
     for col in ['quantity', 'discount', 'profit', 'shipping_cost']:
         df = df.withColumn(col, f.col(col).cast('decimal(12,2)'))
         df = df.withColumn(col, f.when(f.col(col).isNull(), f.lit(0.00).cast('decimal(12,2)')).otherwise(f.col(col)))
     ```
   - **Coluna `sales`**:
     - Registros com valores n√£o num√©ricos s√£o filtrados usando uma express√£o regular (`^-?\d+(\.\d+)?$`).
     - Convertida para `decimal(12,2)`.
     ```python
     sales_regex_clean = r'^-?\d+(\.\d+)?$'
     df = df.filter(f.col("sales").rlike(sales_regex_clean)).withColumn('sales', f.col('sales').cast('decimal(12,2)'))
     ```
   - **Colunas de String**:
     - Valores nulos s√£o substitu√≠dos por `'N/A'`.
     - Espa√ßos em branco nas extremidades s√£o removidos com `trim`.
     ```python
     for col in df.dtypes:
         if col[1] == 'string':
             df = df.withColumn(col[0], f.when(f.col(col[0]).isNull(), f.lit('N/A')).otherwise(f.trim(f.col(col[0]))))
     ```

5. **Persist√™ncia dos Dados**:
   - Os dados refinados s√£o salvos como uma tabela Delta no modo `overwrite`, substituindo qualquer vers√£o anterior.
   ```python
   df.write.format("delta").mode("overwrite").saveAsTable(f"{dest_catalog_name}.{dest_schema_name}.{dest_table_name}")
   ```

## Estrutura da Tabela Silver

A tabela `silver.refined.global_superstore` cont√©m os mesmos 24 campos da tabela Bronze, mas com tipos de dados refinados.


# Camada Gold - An√°lises do Global Superstore Dataset

## Vis√£o Geral

A camada Gold do pipeline de dados no Databricks processa os dados refinados da tabela `silver.refined.global_superstore` para gerar vis√µes anal√≠ticas otimizadas para an√°lises de neg√≥cio. Os resultados s√£o armazenados em tabelas Delta no cat√°logo `gold.analytics` e em arquivos Parquet particionados em `/Volumes/gold/analytics/parquets_files`. Este pipeline calcula m√©tricas como vendas anuais por produto, vendas mensais por categoria e valor total por pedido, suportando decis√µes estrat√©gicas baseadas em dados.

## Objetivo

- Transformar dados refinados da camada Silver em vis√µes agregadas para an√°lises de neg√≥cio.
- Gerar m√©tricas de vendas, quantidades e valores m√©dios, organizadas por produto, categoria e pedido.
- Persistir os resultados em tabelas Delta e arquivos Parquet, otimizados para consultas anal√≠ticas.

## Estrutura do Projeto

- **Fonte de Dados**: Tabela `silver.refined.global_superstore`, contendo dados limpos e tipados.
- **Destino**:
  - Tabelas Delta: Cat√°logo `gold.analytics`.
  - Arquivos Parquet: Diret√≥rio `/Volumes/gold/analytics/parquets_files`.
- **Notebooks**:
  - `nb_gold_annual_metrics_delta`: M√©tricas anuais por produto (tabela Delta).
  - `nb_gold_annual_metrics_parquet`: M√©tricas anuais por produto (arquivo Parquet).
  - `nb_gold_category_delta`: Vendas mensais por categoria (tabela Delta).
  - `nb_gold_category_parquet`: Vendas mensais por categoria (arquivo Parquet).
  - `nb_gold_total_order_value_delta`: Valor total por pedido (tabela Delta).
  - `nb_gold_total_order_value_parquet`: Valor total por pedido (arquivo Parquet).

## Configura√ß√£o

### Par√¢metros
Os par√¢metros s√£o definidos nos notebooks e incluem:

- **Origem (Camada Silver)**:
  - Cat√°logo: `silver`
  - Esquema: `refined`
  - Tabela: `global_superstore`
- **Destino (Camada Gold)**:
  - Cat√°logo: `gold`
  - Esquema: `analytics`
  - Prefixo das tabelas: `global_superstore`
  - Diret√≥rio de Parquet: `/Volumes/gold/analytics/parquets_files`

### Estrutura de Armazenamento
- **Tabelas Delta**: Armazenadas em `gold.analytics` com nomes como `global_superstore_annual_metrics`, `global_superstore_category` e `global_superstore_total_order_value`.
- **Arquivos Parquet**: Armazenados em `/Volumes/gold/analytics/parquets_files`, com particionamento por `year` ou `month` quando aplic√°vel.

## L√≥gica do Pipeline

Os notebooks da camada Gold seguem uma estrutura comum:

1. **Carregamento de Configura√ß√µes**:
   - O notebook `nb_schemas` √© executado para obter os par√¢metros de origem.
   ```python
   %run ./../../ingestion/structs/nb_schemas
   ```

2. **Cria√ß√£o de Estruturas**:
   - Cria√ß√£o do cat√°logo `gold`, esquema `analytics` e volume `parquets_files`, se necess√°rio.
   ```sql
   CREATE CATALOG IF NOT EXISTS gold;
   CREATE SCHEMA IF NOT EXISTS gold.analytics;
   CREATE VOLUME IF NOT EXISTS gold.analytics.parquets_files;
   ```

3. **Leitura dos Dados**:
   - Os dados s√£o lidos da tabela `silver.refined.global_superstore`.
   ```python
   df = spark.read.table(f"{src_catalog_name}.{src_schema_name}.{src_table_name}")
   ```

4. **Transforma√ß√µes e Persist√™ncia**:
   Cada notebook realiza agrega√ß√µes espec√≠ficas e persiste os resultados em tabelas Delta ou arquivos Parquet. Detalhes abaixo:

### 1. M√©tricas Anuais por Produto
- **Notebooks**: `nb_gold_annual_metrics_delta`, `nb_gold_annual_metrics_parquet`
- **Descri√ß√£o**: Calcula m√©tricas anuais por produto, incluindo total de vendas, quantidade vendida e m√©dia de vendas.
- **Sa√≠da**:
  - **Delta**: Tabela `gold.analytics.global_superstore_annual_metrics` (modo `overwrite`, com `overwriteSchema` habilitado).
  - **Parquet**: Arquivos em `/Volumes/gold/analytics/parquets_files/annual_metrics`, particionados por `year` (coalesced para 1 parti√ß√£o).

### 2. Vendas Mensais por Categoria
- **Notebooks**: `nb_gold_category_delta`, `nb_gold_category_parquet`
- **Descri√ß√£o**: Calcula o total de vendas por categoria e m√™s.
- **Sa√≠da**:
  - **Delta**: Tabela `gold.analytics.global_superstore_category` (modo `overwrite`, com `overwriteSchema` habilitado).
  - **Parquet**: Arquivos em `/Volumes/gold/analytics/parquets_files/category`, particionados por `month` (coalesced para 1 parti√ß√£o).

### 3. Valor Total por Pedido
- **Notebooks**: `nb_gold_total_order_value_delta`, `nb_gold_total_order_value_parquet`
- **Descri√ß√£o**: Calcula o valor total de cada pedido.
- **Sa√≠da**:
  - **Delta**: Tabela `gold.analytics.global_superstore_total_order_value` (modo `overwrite`).
  - **Parquet**: Arquivos em `/Volumes/gold/analytics/parquets_files/total_order_value` (coalesced para 1 parti√ß√£o, sem particionamento).

## Notas

- **Performance**: O uso de `coalesce(1)` nos arquivos Parquet reduz o n√∫mero de parti√ß√µes para facilitar a leitura em sistemas externos, mas pode impactar a performance em grandes datasets. Considere ajustar para datasets maiores.
- **Escalabilidade**: A estrutura dos notebooks permite a adi√ß√£o de novas m√©tricas anal√≠ticas, mantendo os par√¢metros e padr√µes de armazenamento.
- **Manuten√ß√£o**: Monitore a tabela Silver para garantir que os dados de origem estejam atualizados antes de executar os notebooks Gold.
